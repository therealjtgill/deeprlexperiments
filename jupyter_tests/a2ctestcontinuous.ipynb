{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FfAgentContinuous(object):\n",
    "    def __init__(self, session, input_size, output_size, gamma=0.99):\n",
    "        self.session = session\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.observations_ph = tf.placeholder(dtype=tf.float32, shape=[None, self.input_size])\n",
    "        # expected sum of discounted rewards\n",
    "        self.esdr_ph = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "        self.v_s_ph  = tf.placeholder(dtype=tf.float32, shape=[None, 1]) # V(s)\n",
    "        self.v_sp_ph = tf.placeholder(dtype=tf.float32, shape=[None, 1]) # V(s')\n",
    "        self.r_ph    = tf.placeholder(dtype=tf.float32, shape=[None, 1]) # r_t+1\n",
    "        self.actions_ph = tf.placeholder(dtype=tf.float32, shape=[None, self.output_size])\n",
    "        #self.reward_ph = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "        \n",
    "        advantage = self.esdr_ph - self.v_s_ph\n",
    "        mean_adv = tf.reduce_mean(advantage)\n",
    "        stddev_adv = tf.sqrt(tf.reduce_mean(advantage*advantage) - mean_adv*mean_adv)\n",
    "        self.adv_normalized = (advantage - mean_adv)/(stddev_adv + 1e-8)\n",
    "        \n",
    "        # Policy network\n",
    "        W1p = tf.get_variable(\"w1p\", [self.input_size, 128], initializer=tf.initializers.random_normal(stddev=0.01))\n",
    "        b1p = tf.get_variable(\"b1p\", [128], initializer=tf.initializers.random_normal(stddev=0.01))\n",
    "        W2p_means = tf.get_variable(\"w2pmeans\", [128, self.output_size], initializer=tf.initializers.random_normal(stddev=0.01)) # policy\n",
    "        W2p_stdevs = tf.get_variable(\"w2pstdevs\", [128, self.output_size], initializer=tf.initializers.random_normal(stddev=0.01)) # policy\n",
    "        b2p_means = tf.get_variable(\"b2pmeans\", [self.output_size], initializer=tf.initializers.random_normal(stddev=0.01))\n",
    "        b2p_stdevs = tf.get_variable(\"b2pstdevs\", [self.output_size], initializer=tf.initializers.random_normal(stddev=0.01))\n",
    "        \n",
    "        l1p = tf.nn.relu(tf.matmul(self.observations_ph, W1p) + b1p)\n",
    "        # this will need to be changed to accommodate the range and character of action values\n",
    "        l2p_means = tf.matmul(l1p, W2p_means) + b2p_means\n",
    "        # Trying to start with a large standard deviation to encourage exploration early on.\n",
    "        l2p_stdevs = tf.matmul(l1p, W2p_stdevs) + b2p_stdevs + 7\n",
    "        l2_policy_means = 2*tf.nn.tanh(l2p_means)\n",
    "        l2_policy_stdevs = tf.math.minimum(tf.nn.softplus(l2p_stdevs), 10) + 0.01\n",
    "        \n",
    "        # Critic network\n",
    "        W1v = tf.get_variable(\"w1v\", [self.input_size, 128], initializer=tf.initializers.random_normal(stddev=0.01))\n",
    "        b1v = tf.get_variable(\"b1v\", [128], initializer=tf.initializers.random_normal(stddev=0.01))\n",
    "        W2v = tf.get_variable(\"w2v\", [128, 1], initializer=tf.initializers.random_normal(stddev=0.01)) # value\n",
    "        b2v = tf.get_variable(\"b2v\", [1], initializer=tf.initializers.random_normal(stddev=0.01))\n",
    "        \n",
    "        l1v = tf.nn.relu(tf.matmul(self.observations_ph, W1v) + b1v)\n",
    "        l2v = tf.matmul(l1v, W2v) + b2v\n",
    "        \n",
    "        self.reinforce_loss = tf.reduce_mean(\n",
    "            (self.esdr_ph)*tf.square((self.actions_ph - l2_policy_means)/(l2_policy_stdevs + 1e-8))\n",
    "            - 0.01*tf.log(l2_policy_stdevs)\n",
    "        )\n",
    "        self.reinforce_optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(self.reinforce_loss)\n",
    "        \n",
    "        self.actor_loss = tf.reduce_mean(\n",
    "            (advantage)*tf.square((self.actions_ph - l2_policy_means)/(l2_policy_stdevs + 1e-8)\n",
    "        ) - 0.01*tf.log(l2_policy_stdevs))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        grads_and_vars = optimizer.compute_gradients(self.actor_loss)\n",
    "        capped_grads = [(grad if grad is None else tf.clip_by_norm(grad, 2.0), var) for grad, var in grads_and_vars]\n",
    "        self.actor_optimizer = optimizer.apply_gradients(capped_grads)\n",
    "        \n",
    "        self.critic_loss = tf.reduce_mean(\n",
    "            tf.square(l2v - self.esdr_ph)\n",
    "        )\n",
    "        self.critic_optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(self.critic_loss)\n",
    "        \n",
    "        self.action_prediction_means = l2_policy_means\n",
    "        self.action_prediction_stdevs = l2_policy_stdevs\n",
    "        self.esdr_predictions = l2v\n",
    "        \n",
    "    # For advantage:\n",
    "    #    Add single timestep reward samples\n",
    "    #    Add placeholders for estimated V(s) and V(s')\n",
    "    def trainSarBatches(self, states, actions, discounted_rewards):\n",
    "        '''\n",
    "        Expects inputs to be numpy arrays of shape:\n",
    "            states = [batch_size, num_state_features]\n",
    "            actions = [batch_size, num_available_actions]\n",
    "            discounted_rewards = [batch_size, 1]\n",
    "        \n",
    "        The idea is that all episodes have been parsed through and shuffled into\n",
    "        one big batch of training data.\n",
    "        '''\n",
    "        \n",
    "        advantage_feeds = {\n",
    "            self.observations_ph: states\n",
    "        }\n",
    "        #print(\"shape of discounted rewards:\", discounted_rewards.shape)\n",
    "        \n",
    "        advantage_fetches = self.esdr_predictions\n",
    "        \n",
    "        v_predictions = self.session.run(advantage_fetches, feed_dict=advantage_feeds)\n",
    "        #print(v_predictions.shape)\n",
    "        \n",
    "        optimize_feeds = {\n",
    "            self.observations_ph: states,\n",
    "            self.esdr_ph: discounted_rewards,\n",
    "            self.v_s_ph: v_predictions,\n",
    "            self.actions_ph: actions\n",
    "        }\n",
    "        \n",
    "        optimize_fetches = [\n",
    "            #self.reinforce_loss,\n",
    "            self.actor_loss,\n",
    "            self.action_prediction_means,\n",
    "            self.action_prediction_stdevs,\n",
    "            self.esdr_predictions,\n",
    "            #self.reinforce_optimizer\n",
    "            #self.actor_critic_optimizer\n",
    "            self.actor_optimizer,\n",
    "            self.critic_optimizer\n",
    "        ]\n",
    "        \n",
    "        loss, action_prediction_means, action_prediction_stdevs, esdr_predictions, _1, _2 = self.session.run(optimize_fetches, feed_dict=optimize_feeds)\n",
    "        return loss, action_prediction_means, action_prediction_stdevs, esdr_predictions\n",
    "    \n",
    "    def predict(self, state):\n",
    "        '''\n",
    "        Expects state to have the shape [num_state_features]\n",
    "        '''\n",
    "        \n",
    "        feeds = {\n",
    "            self.observations_ph: np.array([state])\n",
    "        }\n",
    "        #print(\"state received by agent:\", state)\n",
    "        fetches = [\n",
    "            self.action_prediction_means,\n",
    "            self.action_prediction_stdevs,\n",
    "            self.esdr_predictions\n",
    "        ]\n",
    "        action_prediction_means, action_prediction_stdevs, esdr_predictions = self.session.run(fetches, feed_dict=feeds)\n",
    "        return action_prediction_means, action_prediction_stdevs, esdr_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepSarData(states, actions, rewards, gamma=0.99):\n",
    "    '''\n",
    "    Converts temporally synced lists of states, actions, and rewards into shuffled\n",
    "    numpy matrices for training.\n",
    "    '''\n",
    "    #print(len(states), len(actions), len(rewards))\n",
    "    discounted_sum_rewards = 0\n",
    "    discounted_rewards = []\n",
    "    for i in range(len(rewards) - 1, -1, -1):\n",
    "        discounted_sum_rewards = gamma*discounted_sum_rewards + rewards[i]\n",
    "        discounted_rewards.append(discounted_sum_rewards)\n",
    "    discounted_rewards = np.expand_dims(np.array(discounted_rewards[::-1]), axis=1)\n",
    "    \n",
    "    actions = np.array(actions)\n",
    "    states = np.array(states)\n",
    "    indices = [i for i in range(len(actions))]\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    actions_shuffled = actions[indices]\n",
    "    states_shuffled = states[indices]\n",
    "    discounted_rewards_shuffled = discounted_rewards[indices]\n",
    "    \n",
    "    return actions_shuffled, states_shuffled, discounted_rewards_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulateData(env, agent, max_steps=1000, max_rollouts=200):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    for rollout_count in range(max_rollouts):\n",
    "        ep_states = []\n",
    "        ep_actions = []\n",
    "        ep_rewards = []\n",
    "        ep_state_t = env.reset()\n",
    "        ep_states.append(ep_state_t)\n",
    "        for t in range(max_steps):\n",
    "            ep_action_t = np.random.normal(loc=agent.predict(ep_state_t)[0][0], scale=agent.predict(ep_state_t)[1][0])\n",
    "            #print(ep_action_t)\n",
    "            ep_action_t = min(max(ep_action_t, [-2.0]), [2.0])\n",
    "            #print(ep_action_t)\n",
    "            ep_state_tp1, ep_reward_tp1, done, _ = env.step(ep_action_t)\n",
    "\n",
    "            ep_actions.append(ep_action_t)\n",
    "            ep_states.append(ep_state_tp1)\n",
    "            ep_rewards.append(ep_reward_tp1)\n",
    "            if done:\n",
    "                ep_states.pop(-1)\n",
    "                #ep_rewards.pop(-1)\n",
    "                break\n",
    "            ep_state_t = ep_state_tp1\n",
    "        states.append(ep_states)\n",
    "        actions.append(ep_actions)\n",
    "        rewards.append(ep_rewards)\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renderAgent(env, agent):\n",
    "    state_t = env.reset()\n",
    "    rewards = 0\n",
    "    actions = []\n",
    "    while i < 1000:\n",
    "        action_t = np.random.normal(loc=agent.predict(state_t)[0][0], scale=agent.predict(state_t)[1][0])\n",
    "        #print(ep_action_t)\n",
    "        action_t = min(max(action_t, [-2.0]), [2.0])\n",
    "        actions.append(action_t)\n",
    "        #print(ep_action_t)\n",
    "        state_tp1, reward_tp1, done, _ = env.step(action_t)\n",
    "        rewards += reward_tp1\n",
    "        env.render()\n",
    "        state_t = state_tp1\n",
    "        if done:\n",
    "            print(\"Rewards from rendering:\", rewards)\n",
    "            break\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(type(gym.envs.registry.all()))\n",
    "env_ids = [espec.id for espec in gym.envs.registry.all()]\n",
    "for e in sorted(env_ids):\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendulum = gym.make(\"Pendulum-v0\")\n",
    "session = tf.Session()\n",
    "print(pendulum.observation_space.shape)\n",
    "print(pendulum.action_space)\n",
    "num_actions = len(pendulum.action_space.high)\n",
    "agent = FfAgentContinuous(session, pendulum.observation_space.shape[0], num_actions)\n",
    "\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "average_rewards = []\n",
    "average_stdevs = []\n",
    "for i in range(10000):\n",
    "    states, actions, rewards = accumulateData(pendulum, agent)\n",
    "    #print(actions[0:10])\n",
    "    #print(rewards[0:10])\n",
    "    states_pro = []\n",
    "    actions_pro = []\n",
    "    rewards_pro = []\n",
    "    if i % 10 == 0 and i > 0:\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(average_stdevs)\n",
    "        plt.title(\"Average stdevs so far\")\n",
    "        plt.figure()\n",
    "        plt.plot(average_rewards)\n",
    "        plt.title(\"average rewards so far\")\n",
    "        \n",
    "        plottable_actions = renderAgent(pendulum, agent)\n",
    "        plt.figure()\n",
    "        plt.scatter(range(len(plottable_actions)), plottable_actions)\n",
    "        plt.title(\"Actions Taken in Rendered Environment\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    for j in range(len(actions)):\n",
    "        ret = prepSarData(actions[j], states[j], rewards[j])\n",
    "        mean_reward = np.average(ret[1])\n",
    "        stdev_reward = np.std(ret[1])\n",
    "        states_pro.append(ret[0])\n",
    "        #actions_pro.append((ret[1] - mean_reward)/stdev_reward)\n",
    "        actions_pro.append(ret[1])\n",
    "        rewards_pro.append(ret[2])\n",
    "        #print(ret[0].shape, ret[1].shape, ret[2].shape)\n",
    "        #for k in range(10):\n",
    "        #    agent.trainSarBatches(ret[0], ret[1], ret[2])\n",
    "    for k in range(5*len(states_pro)):\n",
    "        train_index = np.random.choice(a=range(len(states_pro)))\n",
    "        ret = agent.trainSarBatches(states_pro[train_index], actions_pro[train_index], rewards_pro[train_index])\n",
    "        if np.isnan(ret[0]):\n",
    "            print(\"Received nan loss, stopping training.\")\n",
    "            pendulum.close()\n",
    "            sys.exit(-1)\n",
    "    print(i)\n",
    "    average_reward = np.average([sum(r) for r in rewards])\n",
    "    print(\"average reward: \", average_reward, \"stdevs:\", np.average(np.squeeze(ret[2])), \"losses:\", np.average(np.squeeze(ret[0])))\n",
    "    average_stdevs.append(np.average(np.squeeze(ret[2])))\n",
    "    average_rewards.append(average_reward)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(average_rewards)\n",
    "plt.show()\n",
    "pendulum.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pendulum.close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(tf.math.maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(tf.sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
